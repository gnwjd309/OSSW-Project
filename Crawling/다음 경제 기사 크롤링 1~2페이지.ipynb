{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1bshgLolMS5"
      },
      "source": [
        "pip install newspaper3k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUIl3Ca_lPAp"
      },
      "source": [
        "pip install jpype1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEDW13RQlO5H"
      },
      "source": [
        "pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G1FDnivlOv2"
      },
      "source": [
        "import requests\n",
        "from lxml.html import fromstring\n",
        "from newspaper import Article"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMHPGJTdlOKv"
      },
      "source": [
        "url = 'https://news.daum.net/breakingnews/economic'\n",
        "url2 = 'https://news.daum.net/breakingnews/economic?page=2'\n",
        "res = requests.get(url)\n",
        "res2 = requests.get(url2)\n",
        "\n",
        "parser = fromstring(res.text)\n",
        "parser2 = fromstring(res2.text)\n",
        "\n",
        "article_list = parser.xpath('//div[@class=\"box_etc\"]')\n",
        "article_list2 = parser2.xpath('//div[@class=\"box_etc\"]')\n",
        "\n",
        "parsed_articles = article_list[0].xpath('.//li')\n",
        "parsed_articles2 = article_list2[0].xpath('.//li')\n",
        "\n",
        "print(parsed_articles2)\n",
        "print(parsed_articles2)\n",
        "\n",
        "links = []\n",
        "for article in parsed_articles:\n",
        "    parsed_link = article.xpath('.//a[@href]')\n",
        "    print(parsed_link)\n",
        "    link = parsed_link[0].get('href')\n",
        "    links.append(link)\n",
        "print(links)\n",
        "\n",
        "links2 = []\n",
        "for article2 in parsed_articles2:\n",
        "    parsed_link2 = article2.xpath('.//a[@href]')\n",
        "    print(parsed_link2)\n",
        "    link = parsed_link2[0].get('href')\n",
        "    links2.append(link)\n",
        "print(links2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kZgvIwRlTbP"
      },
      "source": [
        "for link in links:\n",
        "\n",
        "  article = Article(link, language='ko')\n",
        "  article.download()\n",
        "  article.parse()\n",
        "  titles = article.title\n",
        "  sentences = article.text\n",
        "  print(\"제목:\",titles)\n",
        "  print(\"내용:\",sentences)\n",
        "\n",
        "for link in links2:\n",
        "\n",
        "  article = Article(link, language='ko')\n",
        "  article.download()\n",
        "  article.parse()\n",
        "  titles = article.title\n",
        "  sentences = article.text\n",
        "  print(\"제목:\",titles)\n",
        "  print(\"내용:\",sentences)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}