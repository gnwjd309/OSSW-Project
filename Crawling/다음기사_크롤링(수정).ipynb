{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "다음기사 크롤링.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1bshgLolMS5"
      },
      "source": [
        "pip install newspaper3k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUIl3Ca_lPAp"
      },
      "source": [
        "pip install jpype1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEDW13RQlO5H"
      },
      "source": [
        "pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G1FDnivlOv2"
      },
      "source": [
        "import requests\n",
        "from lxml.html import fromstring\n",
        "from newspaper import Article"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj87N5MSj7GP"
      },
      "source": [
        "society = []\n",
        "sports = []\n",
        "politics = []\n",
        "economic = []\n",
        "foreign = []\n",
        "culture = []\n",
        "entertain = []\n",
        "digital = []\n",
        "editorial = []\n",
        "press = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMHPGJTdlOKv"
      },
      "source": [
        "for sub in ['society']:\n",
        " for i in range(1,3):\n",
        "     url_page = '?page='\n",
        "     url = 'https://news.daum.net/breakingnews/'+ sub + url_page + str(i)\n",
        "     res = requests.get(url)\n",
        "     \n",
        "     parser = fromstring(res.text)\n",
        " \n",
        "     article_list = parser.xpath('//div[@class=\"box_etc\"]')\n",
        "\n",
        "     parsed_articles = article_list[0].xpath('.//li')\n",
        "\n",
        "     for article in parsed_articles:\n",
        "         parsed_link = article.xpath('.//a[@href]')\n",
        "         \n",
        "         link = parsed_link[0].get('href')\n",
        "         society.append(link)\n",
        "print(society)\n",
        "\n",
        "for sub in ['sports']:\n",
        " for i in range(1,3):\n",
        "     url_page = '?page='\n",
        "     url = 'https://news.daum.net/breakingnews/'+ sub + url_page + str(i)\n",
        "     res = requests.get(url)\n",
        "     \n",
        "     parser = fromstring(res.text)\n",
        " \n",
        "     article_list = parser.xpath('//div[@class=\"box_etc\"]')\n",
        "\n",
        "     parsed_articles = article_list[0].xpath('.//li')\n",
        "\n",
        "     for article in parsed_articles:\n",
        "         parsed_link = article.xpath('.//a[@href]')\n",
        "         \n",
        "         link = parsed_link[0].get('href')\n",
        "         sports.append(link)\n",
        "print(sports)\n",
        "\n",
        "for sub in ['politics']:\n",
        " for i in range(1,3):\n",
        "     url_page = '?page='\n",
        "     url = 'https://news.daum.net/breakingnews/'+ sub + url_page + str(i)\n",
        "     res = requests.get(url)\n",
        "     \n",
        "     parser = fromstring(res.text)\n",
        " \n",
        "     article_list = parser.xpath('//div[@class=\"box_etc\"]')\n",
        "\n",
        "     parsed_articles = article_list[0].xpath('.//li')\n",
        "\n",
        "     for article in parsed_articles:\n",
        "         parsed_link = article.xpath('.//a[@href]')\n",
        "         \n",
        "         link = parsed_link[0].get('href')\n",
        "         politics.append(link)\n",
        "print(politics)\n",
        "\n",
        "for sub in ['economic']:\n",
        " for i in range(1,3):\n",
        "     url_page = '?page='\n",
        "     url = 'https://news.daum.net/breakingnews/'+ sub + url_page + str(i)\n",
        "     res = requests.get(url)\n",
        "     \n",
        "     parser = fromstring(res.text)\n",
        " \n",
        "     article_list = parser.xpath('//div[@class=\"box_etc\"]')\n",
        "\n",
        "     parsed_articles = article_list[0].xpath('.//li')\n",
        "\n",
        "     for article in parsed_articles:\n",
        "         parsed_link = article.xpath('.//a[@href]')\n",
        "         \n",
        "         link = parsed_link[0].get('href')\n",
        "         economic.append(link)\n",
        "print(economic)\n",
        "\n",
        "for sub in ['foreign']:\n",
        " for i in range(1,3):\n",
        "     url_page = '?page='\n",
        "     url = 'https://news.daum.net/breakingnews/'+ sub + url_page + str(i)\n",
        "     res = requests.get(url)\n",
        "     \n",
        "     parser = fromstring(res.text)\n",
        " \n",
        "     article_list = parser.xpath('//div[@class=\"box_etc\"]')\n",
        "\n",
        "     parsed_articles = article_list[0].xpath('.//li')\n",
        "\n",
        "     for article in parsed_articles:\n",
        "         parsed_link = article.xpath('.//a[@href]')\n",
        "         \n",
        "         link = parsed_link[0].get('href')\n",
        "         foreign.append(link)\n",
        "print(foreign)\n",
        "\n",
        "\n",
        "for sub in ['culture']:\n",
        " for i in range(1,3):\n",
        "     url_page = '?page='\n",
        "     url = 'https://news.daum.net/breakingnews/'+ sub + url_page + str(i)\n",
        "     res = requests.get(url)\n",
        "     \n",
        "     parser = fromstring(res.text)\n",
        " \n",
        "     article_list = parser.xpath('//div[@class=\"box_etc\"]')\n",
        "\n",
        "     parsed_articles = article_list[0].xpath('.//li')\n",
        "\n",
        "     for article in parsed_articles:\n",
        "         parsed_link = article.xpath('.//a[@href]')\n",
        "         \n",
        "         link = parsed_link[0].get('href')\n",
        "         culture.append(link)\n",
        "print(culture)\n",
        "\n",
        "\n",
        "for sub in ['entertain']:\n",
        " for i in range(1,3):\n",
        "     url_page = '?page='\n",
        "     url = 'https://news.daum.net/breakingnews/'+ sub + url_page + str(i)\n",
        "     res = requests.get(url)\n",
        "     \n",
        "     parser = fromstring(res.text)\n",
        " \n",
        "     article_list = parser.xpath('//div[@class=\"box_etc\"]')\n",
        "\n",
        "     parsed_articles = article_list[0].xpath('.//li')\n",
        "\n",
        "     for article in parsed_articles:\n",
        "         parsed_link = article.xpath('.//a[@href]')\n",
        "         \n",
        "         link = parsed_link[0].get('href')\n",
        "         entertain.append(link)\n",
        "print(entertain)\n",
        "\n",
        "\n",
        "for sub in ['digital']:\n",
        " for i in range(1,3):\n",
        "     url_page = '?page='\n",
        "     url = 'https://news.daum.net/breakingnews/'+ sub + url_page + str(i)\n",
        "     res = requests.get(url)\n",
        "     \n",
        "     parser = fromstring(res.text)\n",
        " \n",
        "     article_list = parser.xpath('//div[@class=\"box_etc\"]')\n",
        "\n",
        "     parsed_articles = article_list[0].xpath('.//li')\n",
        "\n",
        "     for article in parsed_articles:\n",
        "         parsed_link = article.xpath('.//a[@href]')\n",
        "         \n",
        "         link = parsed_link[0].get('href')\n",
        "         digital.append(link)\n",
        "print(digital)\n",
        "\n",
        "\n",
        "for sub in ['editorial']:\n",
        " for i in range(1,3):\n",
        "     url_page = '?page='\n",
        "     url = 'https://news.daum.net/breakingnews/'+ sub + url_page + str(i)\n",
        "     res = requests.get(url)\n",
        "     \n",
        "     parser = fromstring(res.text)\n",
        " \n",
        "     article_list = parser.xpath('//div[@class=\"box_etc\"]')\n",
        "\n",
        "     parsed_articles = article_list[0].xpath('.//li')\n",
        "\n",
        "     for article in parsed_articles:\n",
        "         parsed_link = article.xpath('.//a[@href]')\n",
        "         \n",
        "         link = parsed_link[0].get('href')\n",
        "         editorial.append(link)\n",
        "print(editorial)\n",
        "\n",
        "\n",
        "\n",
        "for sub in ['press']:\n",
        " for i in range(1,3):\n",
        "     url_page = '?page='\n",
        "     url = 'https://news.daum.net/breakingnews/'+ sub + url_page + str(i)\n",
        "     res = requests.get(url)\n",
        "     \n",
        "     parser = fromstring(res.text)\n",
        " \n",
        "     article_list = parser.xpath('//div[@class=\"box_etc\"]')\n",
        "\n",
        "     parsed_articles = article_list[0].xpath('.//li')\n",
        "\n",
        "     for article in parsed_articles:\n",
        "         parsed_link = article.xpath('.//a[@href]')\n",
        "         \n",
        "         link = parsed_link[0].get('href')\n",
        "         press.append(link)\n",
        "print(press)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kZgvIwRlTbP"
      },
      "source": [
        "for link in society:\n",
        "\n",
        "  article = Article(link, language='ko')\n",
        "  article.download()\n",
        "  article.parse()\n",
        "  titles = article.title\n",
        "  sentences = article.text\n",
        "  print('\\n사회\\n')\n",
        "  print(\"제목:\",titles)\n",
        "  print(\"내용:\",sentences)                           \n",
        "\n",
        "\n",
        "for link in politics:\n",
        "\n",
        "  article = Article(link, language='ko')\n",
        "  article.download()\n",
        "  article.parse()\n",
        "  titles = article.title\n",
        "  sentences = article.text\n",
        "  print('\\n정치\\n')\n",
        "  print(\"제목:\",titles)\n",
        "  print(\"내용:\",sentences) \n",
        "\n",
        "\n",
        "for link in economic:\n",
        "\n",
        "  article = Article(link, language='ko')\n",
        "  article.download()\n",
        "  article.parse()\n",
        "  titles = article.title\n",
        "  sentences = article.text\n",
        "  print('\\n경제\\n')\n",
        "  print(\"제목:\",titles)\n",
        "  print(\"내용:\",sentences) \n",
        "\n",
        "\n",
        "for link in foreign:\n",
        "\n",
        "  article = Article(link, language='ko')\n",
        "  article.download()\n",
        "  article.parse()\n",
        "  titles = article.title\n",
        "  sentences = article.text\n",
        "  print('\\n국제\\n')\n",
        "  print(\"제목:\",titles)\n",
        "  print(\"내용:\",sentences) \n",
        "\n",
        "\n",
        "for link in culture:\n",
        "\n",
        "  article = Article(link, language='ko')\n",
        "  article.download()\n",
        "  article.parse()\n",
        "  titles = article.title\n",
        "  sentences = article.text\n",
        "  print('\\n문화\\n')\n",
        "  print(\"제목:\",titles)\n",
        "  print(\"내용:\",sentences) \n",
        "\n",
        "\n",
        "for link in entertain:\n",
        "\n",
        "  article = Article(link, language='ko')\n",
        "  article.download()\n",
        "  article.parse()\n",
        "  titles = article.title\n",
        "  sentences = article.text\n",
        "  print('\\n연예\\n')\n",
        "  print(\"제목:\",titles)\n",
        "  print(\"내용:\",sentences) \n",
        "\n",
        "\n",
        "\n",
        "for link in sports:\n",
        "\n",
        "  article = Article(link, language='ko')\n",
        "  article.download()\n",
        "  article.parse()\n",
        "  titles = article.title\n",
        "  sentences = article.text\n",
        "  print('\\n스포츠\\n')\n",
        "  print(\"제목:\",titles)\n",
        "  print(\"내용:\",sentences) \n",
        "\n",
        "\n",
        "\n",
        "for link in digital:\n",
        "\n",
        "  article = Article(link, language='ko')\n",
        "  article.download()\n",
        "  article.parse()\n",
        "  titles = article.title\n",
        "  sentences = article.text\n",
        "  print('\\nIT\\n')\n",
        "  print(\"제목:\",titles)\n",
        "  print(\"내용:\",sentences) \n",
        "\n",
        "\n",
        "for link in editorial:\n",
        "\n",
        "  article = Article(link, language='ko')\n",
        "  article.download()\n",
        "  article.parse()\n",
        "  titles = article.title\n",
        "  sentences = article.text\n",
        "  print('\\n칼럼\\n')\n",
        "  print(\"제목:\",titles)\n",
        "  print(\"내용:\",sentences) \n",
        "\n",
        "\n",
        "\n",
        "for link in press:\n",
        "\n",
        "  article = Article(link, language='ko')\n",
        "  article.download()\n",
        "  article.parse()\n",
        "  titles = article.title\n",
        "  sentences = article.text\n",
        "  print('\\n보도자료\\n')\n",
        "  print(\"제목:\",titles)\n",
        "  print(\"내용:\",sentences) \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}