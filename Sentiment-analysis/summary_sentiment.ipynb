{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python391jvsc74a57bd0bf5e4bb18b4432dc119549c9f14bce46c7eb6466365763830a1bf21174326e9a",
   "display_name": "Python 3.9.1 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "bf5e4bb18b4432dc119549c9f14bce46c7eb6466365763830a1bf21174326e9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import jpype\n",
    "except:\n",
    "    import jpype\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import csv\n",
    "import requests\n",
    "from lxml.html import fromstring\n",
    "from newspaper import Article"
   ]
  },
  {
   "source": [
    "## CATEGORY\n",
    "society: 사회 / sports: 스포츠 / politics: 정치 / economic: 경제 / foreign: 국제 / culture: 문화 / entertain: 연예 / digital: IT / editorial: 칼럼 / press: 보도자료"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "{CATEGORY}_texts: 카테고리 + 제목 + 내용\n",
    "\n",
    "{CATEGORY}_summary: 요약\n",
    "\n",
    "{CATEGORY}_keywords: 키워드\n",
    "\n",
    "{CATEGORY}_sentiment: 감정분석"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "society = []\n",
    "sports = []\n",
    "politics = []\n",
    "economic = []\n",
    "foreign = []\n",
    "culture = []\n",
    "entertain = []\n",
    "digital = []\n",
    "editorial = []\n",
    "press = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_link(category):\n",
    "    for sub in ['category']:\n",
    "        for i in range(1,3):\n",
    "            url_page = '?page='\n",
    "            url = 'https://news.daum.net/breakingnews/' + sub + url_page + str(i)\n",
    "            res = requests.get(url)\n",
    "            \n",
    "            parser = fromstring(res.text)\n",
    "        \n",
    "            article_list = parser.xpath('//div[@class=\"box_etc\"]')\n",
    "\n",
    "            parsed_articles = article_list[0].xpath('.//li')\n",
    "\n",
    "            for article in parsed_articles:\n",
    "                parsed_link = article.xpath('.//a[@href]')\n",
    "                \n",
    "                link = parsed_link[0].get('href')\n",
    "                category.append(link)\n",
    "    return category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['https://v.daum.net/v/20210510190429775',\n",
       " 'https://v.daum.net/v/20210510190419774',\n",
       " 'https://v.daum.net/v/20210510190418773',\n",
       " 'https://v.daum.net/v/20210510190417772',\n",
       " 'https://v.daum.net/v/20210510190410771',\n",
       " 'https://v.daum.net/v/20210510190409770',\n",
       " 'https://v.daum.net/v/20210510190408769',\n",
       " 'https://v.daum.net/v/20210510190405767',\n",
       " 'https://v.daum.net/v/20210510190402766',\n",
       " 'https://v.daum.net/v/20210510190401765',\n",
       " 'https://v.daum.net/v/20210510190401764',\n",
       " 'https://v.daum.net/v/20210510190344763',\n",
       " 'https://v.daum.net/v/20210510190342762',\n",
       " 'https://v.daum.net/v/20210510190339761',\n",
       " 'https://v.daum.net/v/20210510190326760',\n",
       " 'https://v.daum.net/v/20210510190429775',\n",
       " 'https://v.daum.net/v/20210510190419774',\n",
       " 'https://v.daum.net/v/20210510190418773',\n",
       " 'https://v.daum.net/v/20210510190417772',\n",
       " 'https://v.daum.net/v/20210510190410771',\n",
       " 'https://v.daum.net/v/20210510190409770',\n",
       " 'https://v.daum.net/v/20210510190408769',\n",
       " 'https://v.daum.net/v/20210510190405767',\n",
       " 'https://v.daum.net/v/20210510190402766',\n",
       " 'https://v.daum.net/v/20210510190401765',\n",
       " 'https://v.daum.net/v/20210510190401764',\n",
       " 'https://v.daum.net/v/20210510190344763',\n",
       " 'https://v.daum.net/v/20210510190342762',\n",
       " 'https://v.daum.net/v/20210510190339761',\n",
       " 'https://v.daum.net/v/20210510190326760']"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "news_link(society)\n",
    "news_link(sports)\n",
    "news_link(politics)\n",
    "news_link(economic)\n",
    "news_link(foreign)\n",
    "news_link(culture)\n",
    "news_link(entertain)\n",
    "news_link(digital)\n",
    "news_link(editorial)\n",
    "news_link(press)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(category, category_ko):\n",
    "    texts = []\n",
    "    sentences = []\n",
    "    for link in category:\n",
    "        article = Article(link, language='ko')\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        titles = article.title\n",
    "        sentence = article.text\n",
    "        text = \"<\" + category_ko + \">\" + \"제목:\" + titles + \"내용:\" + sentence\n",
    "        texts.append(text)\n",
    "        sentences.append(sentence)\n",
    "    return texts, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "society_texts, society_sentences = split(society, '사회')\n",
    "sports_texts, sports_sentences = split(sports, '스포츠')\n",
    "politics_texts, politics_sentences = split(politics, '정치')\n",
    "economic_texts, economic_sentences = split(economic, '경제')\n",
    "foreign_texts, foreign_sentences = split(foreign, '국제')\n",
    "culture_texts, culture__sentences = split(culture, '문화')\n",
    "split_texts, split_sentences = split(entertain, '연예')\n",
    "digital_texts, digital_sentences = split(digital, 'IT')\n",
    "split_texts, split_sentences = split(editorial, '칼럼')\n",
    "press_texts, press_sentences = split(press, '보도자료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(_sentences, words):\n",
    "    ## GraphMatrix\n",
    "    tfidf = TfidfVectorizer()\n",
    "    cnt_vec = CountVectorizer()\n",
    "    sentence_graph = []\n",
    "\n",
    "    # build_sent_graph\n",
    "    tfidf_mat = tfidf.fit_transform(words).toarray()\n",
    "    sentence_graph = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "\n",
    "    # build_words_graph\n",
    "    data = cnt_vec.fit_transform(words).toarray()\n",
    "    cnt_vec_mat = normalize(data, axis=0)\n",
    "    vocab = cnt_vec.vocabulary_\n",
    "    words_graph = np.dot(cnt_vec_mat.T, cnt_vec_mat)\n",
    "    idx2word = {vocab[word] : word for word in vocab}\n",
    "    \n",
    "    ## Rank\n",
    "    # sentence graph\n",
    "    A = sentence_graph\n",
    "    d = 0.85\n",
    "    matrix_size = A.shape[0]\n",
    "\n",
    "    for id in range(matrix_size):\n",
    "        A[id, id] = 0 # diagonal 부분을 0으로\n",
    "        link_sum = np.sum(A[:,id]) # A[:, id] = A[:][id]\n",
    "        if link_sum != 0:\n",
    "                A[:, id] /= link_sum\n",
    "        A[:, id] *= -d\n",
    "        A[id, id] = 1\n",
    "\n",
    "    B = (1-d) * np.ones((matrix_size, 1))\n",
    "    ranks = np.linalg.solve(A, B) # 연립방정식 Ax = b\n",
    "\n",
    "    sentence_graph_idx = {idx: r[0] for idx, r in enumerate(ranks)}\n",
    "\n",
    "    sorted_sentence_rank_idx = sorted(sentence_graph_idx, key=lambda k: sentence_graph_idx[k], reverse=True)\n",
    "\n",
    "    # word graph\n",
    "    A = words_graph\n",
    "    d = 0.85\n",
    "    matrix_size = A.shape[0]\n",
    "\n",
    "    for id in range(matrix_size):\n",
    "        A[id, id] = 0 # diagonal 부분을 0으로\n",
    "        link_sum = np.sum(A[:,id]) # A[:, id] = A[:][id]\n",
    "        if link_sum != 0:\n",
    "                A[:, id] /= link_sum\n",
    "        A[:, id] *= -d\n",
    "        A[id, id] = 1\n",
    "\n",
    "    B = (1-d) * np.ones((matrix_size, 1))\n",
    "    ranks = np.linalg.solve(A, B) # 연립방정식 Ax = b\n",
    "\n",
    "    words_graph_idx = {idx: r[0] for idx, r in enumerate(ranks)}\n",
    "\n",
    "    sorted_words_rank_idx = sorted(words_graph_idx, key=lambda k: words_graph_idx[k], reverse=True)\n",
    "\n",
    "    # summarize\n",
    "    sent_num=3\n",
    "    summary = []\n",
    "    index=[]\n",
    "\n",
    "    for idx in sorted_sentence_rank_idx[:sent_num]:\n",
    "        index.append(idx)\n",
    "\n",
    "    index.sort()\n",
    "    for idx in index:\n",
    "        summary.append(_sentences[idx])\n",
    "\n",
    "    # keywords\n",
    "    word_num=10\n",
    "    keywords = []\n",
    "    index=[]\n",
    "    summarys=[]\n",
    "\n",
    "    for idx in sorted_words_rank_idx[:word_num]:\n",
    "        index.append(idx)\n",
    "        \n",
    "    #index.sort()\n",
    "    for idx in index:\n",
    "        keywords.append(idx2word[idx])\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    for row in summary:\n",
    "        print(row)\n",
    "        summarys.append(row)\n",
    "        print()\n",
    "        count = count + 1\n",
    "        if(count > 3):\n",
    "            break\n",
    "    \n",
    "    print('keywords :', keywords)\n",
    "    return summarys, keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(start, end, _sentences):\n",
    "    table = dict()\n",
    "    with open('f:/eunjin/OSSW-Project/Sentiment-analysis/polarity.csv', 'r', -1, 'utf-8') as polarity:\n",
    "        next(polarity)\n",
    "        \n",
    "        for line in csv.reader(polarity):\n",
    "            key = str()\n",
    "            for word in line[0].split(';'):\n",
    "                key += word.split('/')[0]\n",
    "            table[key] = {'Neg': line[3], 'Neut': line[4], 'Pos': line[6]}\n",
    "\n",
    "    columns=['negative', 'neutral', 'positive']\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    file_stop_word = open('f:/eunjin/OSSW-Project/Sentiment-analysis/stop_words_file.txt', 'r', -1, 'utf-8')\n",
    "    stop_word = file_stop_word.read()\n",
    "    stop_word_list = []\n",
    "    negative_list = []\n",
    "    neutral_list = []\n",
    "    positive_list = []\n",
    "    for word in stop_word.split(','):\n",
    "        if word not in stop_word_list:\n",
    "            stop_word_list.append(word)\n",
    "            file_stop_word.close()\n",
    "\n",
    "    for i in range(start, end):\n",
    "        words = str(_sentences)\n",
    "        hangul = re.compile('[^ ㄱ-ㅣ가-힣]+')\n",
    "        words = hangul.sub('', words)\n",
    "        words_list = []\n",
    "        for i in words:\n",
    "            if i not in stop_word_list:\n",
    "                words_list.append(i)\n",
    "\n",
    "    negative = 0\n",
    "    neutral = 0\n",
    "    positive = 0\n",
    "\n",
    "    for word in words_list:\n",
    "        if word in table:\n",
    "            negative += float(table[word]['Neg'])\n",
    "            neutral += float(table[word]['Neut'])\n",
    "            positive += float(table[word]['Pos'])\n",
    "  \n",
    "    negative_list.append(negative)\n",
    "    neutral_list.append(neutral)\n",
    "    positive_list.append(positive)\n",
    "\n",
    "    df['negative'] = negative_list\n",
    "    df['neutral'] = neutral_list\n",
    "    df['positive'] = positive_list\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(_sentences):\n",
    "    df = text_processing(0,366, _sentences)\n",
    "\n",
    "    df.to_csv('./result.csv', index=False)\n",
    "\n",
    "    ds = pd.read_csv('./result.csv')\n",
    "    ds == ds.values.max()\n",
    "    ids, cols = np.where(ds == ds.values.max())\n",
    "    list(zip(ids, cols))\n",
    "    sen = [ds.columns[c] for c in cols]\n",
    "    return sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "kkma = Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(category_sentences):\n",
    "    for i in category_sentences:\n",
    "        _sentences = kkma.sentences(i)\n",
    "        \n",
    "        words = []\n",
    "\n",
    "        for j in _sentences:\n",
    "            word = okt.nouns(i)\n",
    "            word_str = ' '.join(word)\n",
    "            words.append(word_str)\n",
    "\n",
    "        sentiment_result = sentiment_analysis(_sentences)\n",
    "        summary_result, keywords = summary(_sentences, words)\n",
    "\n",
    "    return summary_result, keywords, sentiment_result"
   ]
  },
  {
   "source": [
    "society_summary, society_keywords, society_sentiment = data(society_sentences)\n",
    "sports_summary, sports_keywords, sports_sentiment = data(sports_sentences)\n",
    "politics_summary, politics_keywords, politics_sentiment = data(politics_sentences)\n",
    "economic_summary, economic_keywords, economic_sentiment = data(economic_sentences)\n",
    "foreign_summary, foreign_keywords, foreign_sentiment = data(foreign_sentences)\n",
    "culture_summary, culture_keywords, culture_sentiment = data(culture_sentences)\n",
    "entertain_summary, entertain_keywords, entertain_sentiment = data(entertain_sentences)\n",
    "digital_summary, digital_keywords, digital_sentiment = data(digital_sentences)\n",
    "editorial_summary, editorial_keywords, editorial_sentiment = data(editorial_sentences)\n",
    "press_summary, press_keywords, press_sentiment = data(press_sentences)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}